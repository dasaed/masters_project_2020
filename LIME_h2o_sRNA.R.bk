#!/usr/bin/env Rscript
# Some terminology
# OrigRF/Orig RF = Original random forest as found in https://github.com/BioinformaticsLabAtMUN/sRNARanking

# A) PRE-SETUP ----
# 1. Clear console and environment ----

clc <- function() cat("\014") ; clc()
remove(list = ls())

# 2. Install and load libraries ----

# uncomment the following lines if you need to install the libraries
# install.packages("h2o")
# install.packages("lime")
# install.packages("ROCR")
# install.packages("PRROC")
# install.packages("tidyverse")
# install.packages("ggplot2") 
library("lime")       # ML local interpretation
library("h2o")        # ML model building
library("ROCR")       # ML evaluation
library("PRROC")      # ML evaluation
library("tidyverse")
library("ggplot2")

# 3. Load and preprocess data ----

# Training Data
fullTrainingDataSet <- read.csv("./tests/combinedData.csv", header = TRUE)
fullTrainingDataSet[,"Class"] <- as.logical(fullTrainingDataSet[,"Class"]) # guarantees that
                          # our h2o model trains the random forest (RF) as a classification 
                          # tree and not a regression tree

dataSetTrainX <- fullTrainingDataSet[,-(8:9)] # to be used by orig training RF model later on
dataSetTrainY <- fullTrainingDataSet[,(8:9)] 

# Testing Data
slt2dataPos <- read.csv("./testing_datasets/SLT2_Positives.tsv", sep = "\t", header = TRUE)
slt2dataPos$Class <- rep(1,nrow(slt2dataPos))
slt2dataNeg <- read.csv("./testing_datasets/SLT2_Negatives.tsv", sep = "\t", header = TRUE)
slt2dataNeg$Class <- rep(0,nrow(slt2dataNeg))
slt2data <- rbind(slt2dataPos,slt2dataNeg)

ludataPos <- read.csv("./testing_datasets/Lu_Positives.tsv", sep = "\t", header = TRUE)
ludataPos$Class <- rep(1,nrow(ludataPos))
ludataNeg <- read.csv("./testing_datasets/Lu_Negatives.tsv", sep = "\t", header = TRUE)
ludataNeg$Class <- rep(0,nrow(ludataNeg))
ludata <- rbind(ludataPos,ludataNeg)



# 4. Load original model ----

# The original article, titled "Prioritizing bona fide bacterial small RNAs with machine learning 
# classifiers", and the source materials(training data, testing data, R scripts, .rds file, etc.) 
# can all be found in PeerJ under the following url: https://peerj.com/articles/6304/
# The original .rds file can be found in github under the following link:
# https://github.com/BioinformaticsLabAtMUN/sRNARanking


## ..4.1 Load model from rds file ----

origRF <- readRDS("RF_classifier4sRNA.rds")

## ..4.2 Create functions for compatibility  ----

# The original model was built using the randomForest library found in CRAN. However, even though
# LIME is supposed to be model agnostic, it's current R implementation only supports certain
# models out of the box. To extend support of the LIME library to other models, the following
# functions need to be defined for each model type. 

predict_model.randomForest <- function(x, newdata, type, ...) {
  res <- predict(x, newdata = newdata, ...)
  switch(
    type,
    raw = data.frame(Response = ifelse(res[,2] > 0.5, "sRNA", "notSRNA"), 
                     stringsAsFactors = FALSE
    ),
    prob = res 
  )
  print(class(res))
  print(dim(res))
  print(res)
}

model_type.randomForest <- function(x, ...) 'classification'

# B) TRAIN THE NEW H2O MODEL ----
# 1. Intialize H2O and load training data ----

h2o.init(              # Initialize h2o
  nthreads = -1,       # -1 = use all available threads
  max_mem_size = "4G"  # specify the amount of memory to use
)
h2o.removeAll()        # clean up the system, h2o-wise (ie. kill any running h2o clusters)
h2o.no_progress() 
trainData <- as.h2o(fullTrainingDataSet) 

# 2. Build model ----

rfh2o <- h2o.randomForest( 
             training_frame = trainData,
             x = 1:7,                   # features to use to generate the prediction
             y = 9,                     # Class type -> what we want to predict
             model_id = "rf1_sRNA",     # name of model in h2o
             ntrees = 400,              # max number of trees  
             seed = 1234,               # seed, has to be set WITHIN the h2o function
                                        # and it's supposed to be different from "R's seed", so 
                                        # results might not be exactly the same as orig model, but
                                        # should be similar enough
             mtries = 2,                 # Same as original model 
             max_depth = 30
)

# 3. Preview H2O's RF ----
# This is the performance with the OOB error, based on the training process (ie training data)
rfh2o
rfh2o@model$variable_importances
h2o.varimp_plot(rfh2o)
rfh2o_training_peformance <- h2o.performance(rfh2o)
rfh2o_training_peformance

# C) COMPARE MODEL'S PREDICTIONS ----
# In this section we'll get the predictions both models generate on the testing data LU and SLT2. The goal
# of this section is not to prove which model is the best, but to prove that the models are similar enough
# to the point that the analysis of the H2O model will be a valid analogy model for the original RF model. 

# 1. Get Testing Data Predictions from the Models ----

origRF_lu_pred <- predict(origRF, ludata[,-8], type = "prob")
origRF_slt2_pred <- predict(origRF, slt2data[,-8], type = "prob")

rfh2o_slt2_pred <- h2o.predict(object = rfh2o, newdata = as.h2o(slt2data[,-8]) )
rfh2o_lu_pred <- h2o.predict(object = rfh2o, newdata = as.h2o(ludata[,-8]) )

# 2. Create a comparison function ----
# This function is to simplify the comparison between the predictions from the Original RF model and the H2O RF model
compareAnswers <- function(a,b,c){
  if ( a == c & b == c ){
    res="BOTH_RIGHT"
    #res="BOTH"
  }
  else if ( a == c & b != c) {
    res="OnlyA"    
  }
  else if ( a != c & b == c) {
    res="OnlyB"    
  }
  else{
    res="BOTH_WRONG"
    #res="BOTH"  
  }
  return(res)
}

# 3. SLT2 comparisons ----
slt2_predictions <-  cbind(as.data.frame(slt2data),               # Input
                           as.data.frame(origRF_slt2_pred[,2]),   # Orig RF predictions 
                           as.data.frame(rfh2o_slt2_pred[,3])     # Orig RF predictions
                           )

colnames(slt2_predictions) <- c("SS", "Pos10wrtsRNAStart", "DistTerm", "Distance", 
                                "sameStrand", "DownDistance", "sameDownStrand", "Class",
                                "OrigRF_T","RF_H2O_T")

slt2_predictions$origPreds <- ifelse( slt2_predictions$OrigRF_T >= 0.5, 1, 0) 
slt2_predictions$h2oPreds  <- ifelse( slt2_predictions$RF_H2O_T >= 0.5, 1, 0) 
slt2_predictions$Similarities <- NA

for( i in 1:nrow(slt2_predictions) ){
  # Based on the way we are feeding the values to the compareAnswers function, 
  # Similiarities = OnlyA means that only the Orig RF got the right answer
  # Similiarities = OnlyB means that only the H2O RF got the right answer
  # All other answers will tell us where both models were right("BOTH_RIGHT"), or
  # wrong ("BOTH_WRONG")
  slt2_predictions[i,]$Similarities <- compareAnswers(
    slt2_predictions[i,]$origPreds,  
    slt2_predictions[i,]$h2oPreds,  
    slt2_predictions[i,]$Class 
  )
} 

head(slt2_predictions,2)

slt2_OnlyA <- slt2_predictions[slt2_predictions$Similarities == "OnlyA",] 
slt2_OnlyB <- slt2_predictions[slt2_predictions$Similarities == "OnlyB",]

# By looking at the difference between the number of predictions the Orig. RF and the H2O RF,
# we can conclude that the models are similar enough in accuracy
nrow(slt2_OnlyA)
nrow(slt2_OnlyB)

# 4. LU Comparisons ----

lu_predictions <-  cbind(as.data.frame(ludata),               # Input
                           as.data.frame(origRF_lu_pred[,2]),   # Orig RF predictions 
                           as.data.frame(rfh2o_lu_pred[,3])     # Orig RF predictions
)

colnames(lu_predictions) <- c("SS", "Pos10wrtsRNAStart", "DistTerm", "Distance", 
                                "sameStrand", "DownDistance", "sameDownStrand", "Class",
                                "OrigRF_T","RF_H2O_T")

lu_predictions$origPreds <- ifelse( lu_predictions$OrigRF_T >= 0.5, 1, 0) 
lu_predictions$h2oPreds  <- ifelse( lu_predictions$RF_H2O_T >= 0.5, 1, 0) 
lu_predictions$Similarities <- NA

for( i in 1:nrow(lu_predictions) ){
  # Based on the way we are feeding the values to the compareAnswers function, 
  # Similiarities = OnlyA means that only the Orig RF got the right answer
  # Similiarities = OnlyB means that only the H2O RF got the right answer
  # All other answers will tell us where both models were right("BOTH_RIGHT"), or
  # wrong ("BOTH_WRONG")
  lu_predictions[i,]$Similarities <- compareAnswers(
    lu_predictions[i,]$origPreds,  
    lu_predictions[i,]$h2oPreds,  
    lu_predictions[i,]$Class 
  )
} 

head(lu_predictions,2)

lu_OnlyA <- lu_predictions[lu_predictions$Similarities == "OnlyA",] 
lu_OnlyB <- lu_predictions[lu_predictions$Similarities == "OnlyB",]

# By looking at the difference between the number of predictions the Orig. RF and the H2O RF,
# we can conclude that the models are similar enough in accuracy
nrow(lu_OnlyA)
nrow(lu_OnlyB)


# D) COMPARE MODEL'S METRICS ----
# In this section we'll get the model's metrics, and do a side by side comparison of
# the metrics. 
# 1. Get Performance Metrics ----
# This function was copied directly from the source materials, as it was the one used
# to evaluate the original model, and only the comments have been changed
evaluateData <- function(RF, data, labels){
  require(ROCR)
  require(PRROC)
  require(randomForest)
  res <- list()
  res$predD <- predict(RF, data, type = "prob")                        # Predictions as Generated by the model
  res$pred <- prediction(res$predD[,2], as.logical(labels))            # Predictions, but converted to S4 Object          
  res$PR <- performance(res$pred, measure = "prec", x.measure = "rec") # Precision Recall Curve
  res$SS <- performance(res$pred, measure="sens", x.measure="spec")    # Sensitivity vs. Specificity
  res$auc  <- performance(res$pred, measure = "auc")                   # Sensitivity vs. Specificity AUC
  res$acc <-  performance(res$pred, measure = "acc")                   # Accuracy
  res$pr <- pr.curve(scores.class0 = res$predD[labels == 1,2],         # Precision vs. Recall 
                     scores.class1 = res$predD[labels == 0,2], 
                     curve = T)
  return(res)
}

# OrigRF Performance Metrics
origRF_slt2_performance <- evaluateData(origRF, slt2data[,-8], slt2data[,8])
origRF_lu_performance <- evaluateData(origRF, ludata[,-8], ludata[,8])

# H2O RF Performance Metrics
slt2data_h2o <- slt2data
slt2data_h2o[,"Class"] <- as.logical(slt2data_h2o[,"Class"]) 
rfh2o_slt2_performance <- h2o.performance(rfh2o, newdata = as.h2o(slt2data_h2o))
ludata_h2o <- ludata
ludata_h2o[,"Class"] <- as.logical(ludata_h2o[,"Class"]) 
rfh2o_lu_performance <- h2o.performance(rfh2o, newdata = as.h2o(ludata_h2o))

# 2. Compare Metrics  ----
# ..2.1 Accuracy ----

metrics_table <- data.frame("Accuracy" = 1:4)
rownames(metrics_table) <- c("origRF_slt2_perf", "rfh2o_slt2_perf","origRF_lu_perf", "rfh2o_lu_perf")
metrics_table["origRF_slt2_perf","Accuracy"] <- nrow(slt2_predictions[slt2_predictions$Similarities == "BOTH_RIGHT" | 
                                                     slt2_predictions$Similarities == "OnlyA",]
                                    )/nrow(slt2_predictions)
metrics_table["rfh2o_slt2_perf","Accuracy"] <- nrow(slt2_predictions[slt2_predictions$Similarities == "BOTH_RIGHT" | 
                                                       slt2_predictions$Similarities == "OnlyB",]
                                    )/nrow(slt2_predictions)
metrics_table["origRF_lu_perf","Accuracy"] <- nrow(lu_predictions[lu_predictions$Similarities == "BOTH_RIGHT" | 
                                                     lu_predictions$Similarities == "OnlyA",]
                                    )/nrow(lu_predictions)
metrics_table["rfh2o_lu_perf","Accuracy"] <- nrow(lu_predictions[lu_predictions$Similarities == "BOTH_RIGHT" | 
                                                     lu_predictions$Similarities == "OnlyB",]
                                    )/nrow(lu_predictions)

metrics_table

# ..2.2 AUCPR ----

metrics_table["origRF_slt2_perf","AUCPR"] <- origRF_slt2_performance$pr$auc.integral
metrics_table["rfh2o_slt2_perf", "AUCPR"] <- h2o.aucpr(rfh2o_slt2_performance)
metrics_table["origRF_lu_perf",  "AUCPR"] <- origRF_lu_performance$pr$auc.integral
metrics_table["rfh2o_lu_perf",  "AUCPR"] <- h2o.aucpr(rfh2o_lu_performance)

metrics_table

plot(origRF_slt2_performance$pr) 
plot(origRF_lu_performance$pr) 

# ..2.3 AUC ----
plot(rfh2o_slt2_performance, type = "roc")
plot(rfh2o_slt2_performance, type = "cutoffs")

metrics <- as.data.frame(h2o.metric(rfh2o_slt2_performance))

metrics %>%  ggplot(aes(recall,precision)) + geom_line() + theme_minimal()

plot(origRF_slt2_performance$acc)
plot(origRF_lu_performance$acc)

origRF_slt2_performance$SS
plot(origRF_slt2_performance$SS)

origRF_slt2_performance$auc@y.values 
origRF_slt2_performance$PR    
plot(origRF_slt2_performance$PR)

origRF_slt2_performance$acc

# ..2.X Other H2O Metrics ----
h2o.confusionMatrix(rfh2o_slt2_performance)
h2o.confusionMatrix(rfh2o_lu_performance)

# D) LIME ----
# 1. Apply LIME to the new RF models ----
lime_explainer_rfh2o <- lime( as.data.frame(trainData[,c(1:7)]), # original training data
                             rfh2o,
                             bin_continuous = TRUE,
                             quantile_bins = FALSE
                            )

lime_explanations_rfh2o <- explain( as.data.frame(testData_slt2[1,]),   # Data to explain
                              lime_explainer_rfh20,     # Explainer to use
                              n_labels = 1, # only 1 type of category
                              n_features = 7, # Number of features we want to use for explanation
                              n_permutations = 250,
                              feature_select = "none"
)
lime_explanations_rfh2o
plot_features(lime_explanations)



# E) PDP ----
# 1. Generate PDPs for the RF models ----

h2o.partialPlot(rfh2o, data = as.h2o(trainData), cols = "SS")
h2o.partialPlot(rfh2o, data = as.h2o(trainData), cols = "Pos10wrtsRNAStart")
h2o.partialPlot(rfh2o, data = as.h2o(trainData), cols = "DistTerm")
h2o.partialPlot(rfh2o, data = as.h2o(trainData), cols = "Distance")
h2o.partialPlot(rfh2o, data = as.h2o(trainData), cols = "DownDistance")
h2o.partialPlot(rfh2o, data = as.h2o(trainData), cols = "sameStrand")
h2o.partialPlot(rfh2o, data = as.h2o(trainData), cols = "sameDownStrand")


# F) SHAP Values ----
# 1. something ... ----
SHAP_H2O <- h2o.predict_contributions(rfh2o, as.h2o(slt2data[2,-c(6:8)]))
SHAP_H2O
